{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ¦ PredicÈ›ia Riscului de Credit - Analiza Default-ului de Ãmprumut\n",
    "\n",
    "## ğŸ“Š Descrierea Proiectului\n",
    "\n",
    "**Problema RealÄƒ:** Ãn domeniul financiar, bÄƒncile È™i instituÈ›iile de creditare trebuie sÄƒ evalueze riscul ca un client sÄƒ nu Ã®È™i plÄƒteascÄƒ Ã®mprumutul la timp (default). AceastÄƒ predicÈ›ie ajutÄƒ la:\n",
    "- Minimizarea pierderilor financiare\n",
    "- Stabilirea ratelor dobÃ¢nzilor adecvate\n",
    "- Luarea deciziilor de aprobare/respingere a Ã®mprumuturilor\n",
    "\n",
    "**Obiectivul:** Construim un model de Machine Learning care sÄƒ calculeze **scorul de propensiune** (probabilitatea) ca un client sÄƒ intre Ã®n default.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ce Vom ÃnvÄƒÈ›a\n",
    "\n",
    "1. **Clasificare BinarÄƒ** - PredicÈ›ie \"Default\" (1) vs \"No Default\" (0)\n",
    "2. **Modele Structurate:**\n",
    "   - Regresia LogisticÄƒ (interpretabilÄƒ, rapidÄƒ)\n",
    "   - Linear Discriminant Analysis (LDA)\n",
    "3. **K-Nearest Neighbors (KNN)** - Cu standardizare z-scores\n",
    "4. **Evaluare pentru Date Imbalanced:**\n",
    "   - Confusion Matrix\n",
    "   - Precision, Recall, F1-Score\n",
    "   - ROC Curve È™i AUC\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Dataset\n",
    "\n",
    "Vom folosi **German Credit Data** de la UCI Machine Learning Repository - un dataset real folosit Ã®n industrie pentru training Ã®n credit scoring.\n",
    "\n",
    "**Context:** 1000 de clienÈ›i cu 20 de atribute (vÃ¢rstÄƒ, status cont bancar, credit history, ocupaÈ›ie, etc.) È™i rezultatul Ã®mprumutului.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1ï¸âƒ£ Setup È™i Instalare LibrÄƒrii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# InstalÄƒm librÄƒrii suplimentare dacÄƒ e necesar\n",
    "!pip install ucimlrepo --quiet\n",
    "!pip install imbalanced-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Importuri pentru manipularea datelor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Vizualizare\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Machine Learning - Modele\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Machine Learning - Evaluare\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_curve, roc_auc_score, auc,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Pentru date imbalanced\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Pentru accesarea dataset-ului\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "print(\"âœ… Toate librÄƒriile au fost importate cu succes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 2ï¸âƒ£ ÃncÄƒrcarea Datelor Reale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# ÃncÄƒrcÄƒm German Credit Data de la UCI ML Repository\n",
    "print(\"ğŸ“¥ DescÄƒrcÄƒm German Credit Dataset de la UCI...\")\n",
    "\n",
    "# ID-ul pentru German Credit Data Ã®n UCI Repository\n",
    "statlog_german_credit_data = fetch_ucirepo(id=144)\n",
    "\n",
    "# Extragem features (X) È™i target (y)\n",
    "X = statlog_german_credit_data.data.features\n",
    "y = statlog_german_credit_data.data.targets\n",
    "\n",
    "print(f\"âœ… Date Ã®ncÄƒrcate: {X.shape[0]} clienÈ›i, {X.shape[1]} features\")\n",
    "print(f\"\\nğŸ“‹ Primele coloane (Ã®nainte de redenumire):\")\n",
    "print(X.columns.tolist()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_mapping"
   },
   "source": [
    "### ğŸ“ Redenumirea Features-urilor\n",
    "\n",
    "Dataset-ul vine cu atribute generice (Attribute1, Attribute2...). Le vom redenumi cu **numele lor reale** pentru claritate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rename_features"
   },
   "outputs": [],
   "source": [
    "# Dictionary cu maparea dintre Attribute1, Attribute2... È™i numele real\n",
    "feature_names = {\n",
    "    'Attribute1': 'checking_status',          # Status cont curent (A11: <0 DM, A12: 0-200 DM, A13: >200 DM, A14: no account)\n",
    "    'Attribute2': 'duration_months',          # Durata Ã®mprumutului Ã®n luni\n",
    "    'Attribute3': 'credit_history',           # Istoric credit (A30-A34: de la no credits la all paid back)\n",
    "    'Attribute4': 'purpose',                  # Scopul Ã®mprumutului (car, furniture, radio/TV, etc.)\n",
    "    'Attribute5': 'credit_amount',            # Suma Ã®mprumutului\n",
    "    'Attribute6': 'savings',                  # Cont economii/bonds (A61: <100 DM, A62: 100-500, etc.)\n",
    "    'Attribute7': 'employment_duration',      # Durata angajÄƒrii actuale\n",
    "    'Attribute8': 'installment_rate',         # Rata lunarÄƒ ca % din venitul disponibil\n",
    "    'Attribute9': 'personal_status_sex',      # Status personal È™i sex\n",
    "    'Attribute10': 'other_debtors',           # AlÈ›i debitori/garanÈ›i\n",
    "    'Attribute11': 'present_residence',       # Ani la reÈ™edinÈ›a actualÄƒ\n",
    "    'Attribute12': 'property',                # Tipul proprietÄƒÈ›ii (real estate, savings, car, etc.)\n",
    "    'Attribute13': 'age_years',               # VÃ¢rsta Ã®n ani\n",
    "    'Attribute14': 'other_installments',      # Alte planuri de rate (bank, stores, none)\n",
    "    'Attribute15': 'housing',                 # Tipul locuinÈ›ei (rent, own, for free)\n",
    "    'Attribute16': 'existing_credits',        # NumÄƒr de credite existente la aceastÄƒ bancÄƒ\n",
    "    'Attribute17': 'job',                     # Tipul jobului\n",
    "    'Attribute18': 'num_dependents',          # NumÄƒr de persoane Ã®ntreÈ›inute\n",
    "    'Attribute19': 'telephone',               # Are telefon? (yes/no)\n",
    "    'Attribute20': 'foreign_worker'           # LucrÄƒtor strÄƒin? (yes/no)\n",
    "}\n",
    "\n",
    "# Redenumim coloanele\n",
    "X.rename(columns=feature_names, inplace=True)\n",
    "\n",
    "print('âœ… Features redenumite cu succes!\\n')\n",
    "print('ğŸ“‹ Lista completÄƒ de features:')\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f'   {i:2d}. {col}')\n",
    "\n",
    "print('\\nğŸ’¡ Features mai importante de observat:')\n",
    "print('   ğŸ”¹ credit_amount - Suma Ã®mprumutului (numeric)')\n",
    "print('   ğŸ”¹ duration_months - Durata Ã®mprumutului (numeric)')\n",
    "print('   ğŸ”¹ age_years - VÃ¢rsta clientului (numeric)')\n",
    "print('   ğŸ”¹ checking_status - Status cont curent (categorial)')\n",
    "print('   ğŸ”¹ credit_history - Istoricul de credit (categorial)')\n",
    "print('   ğŸ”¹ purpose - Scopul Ã®mprumutului (categorial)')\n",
    "\n",
    "print('\\nğŸ“Š Tipuri de date:')\n",
    "print(X.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_details"
   },
   "source": [
    "### ğŸ“– ExplicaÈ›ii Detaliate pentru Features Categoriale\n",
    "\n",
    "**Features-urile categoriale au coduri specific (A11, A12, etc.). IatÄƒ ce Ã®nseamnÄƒ:**\n",
    "\n",
    "#### ğŸ¦ **checking_status** (Status cont curent):\n",
    "- `A11`: < 0 DM (Ã®n descoperit)\n",
    "- `A12`: 0 - 200 DM\n",
    "- `A13`: >= 200 DM / salary assignments\n",
    "- `A14`: Nu are cont curent\n",
    "\n",
    "#### ğŸ“œ **credit_history** (Istoric credit):\n",
    "- `A30`: No credits taken / all paid back\n",
    "- `A31`: All credits at this bank paid back\n",
    "- `A32`: Existing credits paid back duly till now\n",
    "- `A33`: Delay in paying off in the past\n",
    "- `A34`: Critical account / other existing credits\n",
    "\n",
    "#### ğŸ¯ **purpose** (Scopul Ã®mprumutului):\n",
    "- `A40`: Car (new)\n",
    "- `A41`: Car (used)\n",
    "- `A42`: Furniture/equipment\n",
    "- `A43`: Radio/television\n",
    "- `A44`: Domestic appliances\n",
    "- `A45`: Repairs\n",
    "- `A46`: Education\n",
    "- `A48`: Retraining\n",
    "- `A49`: Business\n",
    "- `A410`: Others\n",
    "\n",
    "#### ğŸ’° **savings** (Cont economii):\n",
    "- `A61`: < 100 DM\n",
    "- `A62`: 100 - 500 DM\n",
    "- `A63`: 500 - 1000 DM\n",
    "- `A64`: >= 1000 DM\n",
    "- `A65`: Unknown / no savings account\n",
    "\n",
    "#### ğŸ’¼ **employment_duration**:\n",
    "- `A71`: Unemployed\n",
    "- `A72`: < 1 year\n",
    "- `A73`: 1 - 4 years\n",
    "- `A74`: 4 - 7 years\n",
    "- `A75`: >= 7 years\n",
    "\n",
    "*DM = Deutsche Mark (moneda Germaniei Ã®nainte de EURO)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_sample_values"
   },
   "outputs": [],
   "source": [
    "# ArÄƒtÄƒm cÃ¢teva exemple de valori pentru features categoriale\n",
    "print(\"ğŸ“‹ Exemple de valori pentru features categoriale:\\n\")\n",
    "\n",
    "categorical_features = ['checking_status', 'credit_history', 'purpose', 'savings', 'employment_duration']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in X.columns:\n",
    "        print(f\"\\nğŸ”¹ {feature}:\")\n",
    "        print(X[feature].value_counts().head())\n",
    "        print(f\"   Total unique values: {X[feature].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eda"
   },
   "source": [
    "## 3ï¸âƒ£ Exploratory Data Analysis (EDA)\n",
    "\n",
    "### ğŸ“Š ÃnÈ›elegerea Datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eda_basic"
   },
   "outputs": [],
   "source": [
    "# CombinÄƒm X È™i y pentru analizÄƒ\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Redenumim coloana target pentru claritate\n",
    "df.rename(columns={'class': 'default'}, inplace=True)\n",
    "\n",
    "print(\"ğŸ” Primele 5 rÃ¢nduri din dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nğŸ“Š InformaÈ›ii despre tipurile de date:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nğŸ“ˆ Statistici descriptive:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "target_analysis"
   },
   "source": [
    "### ğŸ¯ Analiza Variabilei Target - Verificarea Imbalance-ului\n",
    "\n",
    "**IMPORTANT:** Ãn credit scoring, default-ul este o **clasÄƒ rarÄƒ** (minority class). Acest lucru se numeÈ™te **imbalanced data** È™i necesitÄƒ atenÈ›ie specialÄƒ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "target_dist"
   },
   "outputs": [],
   "source": [
    "# TransformÄƒm target Ã®n 0/1 (0 = No Default, 1 = Default)\n",
    "# Ãn dataset-ul original: 1 = Good Credit, 2 = Bad Credit\n",
    "df['default'] = df['default'].map({1: 0, 2: 1})  # 0 = No Default, 1 = Default\n",
    "\n",
    "print(\"ğŸ“Š DistribuÈ›ia variabilei target:\")\n",
    "print(df['default'].value_counts())\n",
    "print(\"\\nğŸ“ˆ ProporÈ›ii:\")\n",
    "print(df['default'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Vizualizare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Countplot\n",
    "sns.countplot(data=df, x='default', ax=axes[0], palette=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('DistribuÈ›ia Default-ului', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Default (0 = Nu, 1 = Da)')\n",
    "axes[0].set_ylabel('NumÄƒr de clienÈ›i')\n",
    "\n",
    "# Pie chart\n",
    "df['default'].value_counts().plot.pie(\n",
    "    ax=axes[1], \n",
    "    autopct='%1.1f%%',\n",
    "    labels=['No Default', 'Default'],\n",
    "    colors=['#2ecc71', '#e74c3c'],\n",
    "    startangle=90\n",
    ")\n",
    "axes[1].set_title('ProporÈ›ia Default-ului', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# CalculÄƒm imbalance ratio\n",
    "default_count = df['default'].sum()\n",
    "no_default_count = len(df) - default_count\n",
    "imbalance_ratio = no_default_count / default_count\n",
    "\n",
    "print(f\"\\nâš ï¸ Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Pentru fiecare client cu default, avem {imbalance_ratio:.2f} clienÈ›i fÄƒrÄƒ default.\")\n",
    "print(\"\\nğŸ’¡ Acest imbalance necesitÄƒ tehnici speciale de evaluare!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "features_analysis"
   },
   "source": [
    "### ğŸ” Analiza Features Importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "features_viz"
   },
   "outputs": [],
   "source": [
    "# SelectÄƒm features numerice pentru vizualizare\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features.remove('default')\n",
    "\n",
    "# VizualizÄƒm distribuÈ›ia pentru primele 6 features numerice\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(numeric_features[:6]):\n",
    "    # Histograma separatÄƒ pentru default vs no default\n",
    "    df[df['default'] == 0][feature].hist(\n",
    "        ax=axes[idx], bins=20, alpha=0.6, label='No Default', color='#2ecc71'\n",
    "    )\n",
    "    df[df['default'] == 1][feature].hist(\n",
    "        ax=axes[idx], bins=20, alpha=0.6, label='Default', color='#e74c3c'\n",
    "    )\n",
    "    axes[idx].set_title(f'DistribuÈ›ia: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('FrecvenÈ›Äƒ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ ObservaÈ›ii:\")\n",
    "print(\"- CautÄƒ diferenÈ›e Ã®n distribuÈ›ii Ã®ntre cele douÄƒ clase\")\n",
    "print(\"- Features cu diferenÈ›e mari sunt predictori potenÈ›iali\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "correlation"
   },
   "source": [
    "### ğŸ“Š Matricea de CorelaÈ›ie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "correlation_matrix"
   },
   "outputs": [],
   "source": [
    "# CorelaÈ›ia pentru features numerice\n",
    "numeric_df = df[numeric_features + ['default']]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True, \n",
    "    fmt='.2f', \n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1\n",
    ")\n",
    "plt.title('Matricea de CorelaÈ›ie - Features Numerice', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cele mai corelate features cu target\n",
    "print(\"\\nğŸ¯ Top features corelate cu DEFAULT (Ã®n valoare absolutÄƒ):\")\n",
    "target_corr = correlation_matrix['default'].abs().sort_values(ascending=False)\n",
    "print(target_corr[1:11])  # Excludem corelaÈ›ia cu el Ã®nsuÈ™i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## 4ï¸âƒ£ Preprocessing È™i PregÄƒtirea Datelor\n",
    "\n",
    "### ğŸ”§ Ce Vom Face:\n",
    "1. Tratarea valorilor lipsÄƒ (dacÄƒ existÄƒ)\n",
    "2. Encoding pentru variabile categoriale\n",
    "3. Split Train/Test\n",
    "4. **Standardizare (Z-scores)** - Crucial pentru KNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_missing"
   },
   "outputs": [],
   "source": [
    "# VerificÄƒm valori lipsÄƒ\n",
    "print(\"ğŸ” VerificÄƒm valori lipsÄƒ:\")\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"âœ… Nu existÄƒ valori lipsÄƒ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "encode_categorical"
   },
   "outputs": [],
   "source": [
    "# Encoding pentru variabile categoriale\n",
    "print(\"ğŸ”„ Encoding variabile categoriale...\")\n",
    "\n",
    "# IdentificÄƒm coloanele categoriale\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Coloane categoriale gÄƒsite: {len(categorical_cols)}\")\n",
    "\n",
    "# Label Encoding pentru categoriale\n",
    "df_encoded = df.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"  âœ“ {col}: {len(le.classes_)} categorii\")\n",
    "\n",
    "print(\"\\nâœ… Encoding completat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_test_split"
   },
   "outputs": [],
   "source": [
    "# SeparÄƒm features È™i target\n",
    "X = df_encoded.drop('default', axis=1)\n",
    "y = df_encoded['default']\n",
    "\n",
    "print(f\"ğŸ“Š Shape features: {X.shape}\")\n",
    "print(f\"ğŸ“Š Shape target: {y.shape}\")\n",
    "\n",
    "# Train/Test Split cu stratificare (pÄƒstrÄƒm proporÈ›ia claselor)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # 20% pentru test\n",
    "    random_state=42,\n",
    "    stratify=y  # â­ IMPORTANT pentru date imbalanced!\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"ğŸ“‰ Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# VerificÄƒm proporÈ›iile Ã®n train È™i test\n",
    "print(\"\\nğŸ” ProporÈ›ia default Ã®n Train:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nğŸ” ProporÈ›ia default Ã®n Test:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "standardization"
   },
   "source": [
    "### âš–ï¸ Standardizare (Z-Scores)\n",
    "\n",
    "**De ce este crucialÄƒ standardizarea?**\n",
    "\n",
    "1. **Pentru KNN:** DistanÈ›a EuclideanÄƒ este sensibilÄƒ la scalarea features:\n",
    "   - Feature cu valori mari (ex: venit 10000-100000) va domina\n",
    "   - Feature cu valori mici (ex: numÄƒr copii 0-5) va fi ignorat\n",
    "\n",
    "2. **Z-Score Formula:** `z = (x - Î¼) / Ïƒ`\n",
    "   - TransformÄƒ fiecare feature sÄƒ aibÄƒ media 0 È™i deviaÈ›ie standard 1\n",
    "   - Toate features ajung pe aceeaÈ™i \"scalÄƒ\"\n",
    "\n",
    "3. **Pentru Logistic Regression È™i LDA:** Nu este strict necesarÄƒ, dar ajutÄƒ la convergenÈ›Äƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "standardize"
   },
   "outputs": [],
   "source": [
    "# Standardizare folosind StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# âš ï¸ IMPORTANT: Fit doar pe train, apoi transform pe ambele seturi!\n",
    "# Aceasta previne \"data leakage\"\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertim Ã®napoi Ã®n DataFrame pentru clarity\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"âœ… Standardizare completatÄƒ!\")\n",
    "print(\"\\nğŸ“Š VerificÄƒm statistici dupÄƒ standardizare (Train):\")\n",
    "print(f\"Media: {X_train_scaled.mean().mean():.6f} (ar trebui ~0)\")\n",
    "print(f\"Std Dev: {X_train_scaled.std().mean():.6f} (ar trebui ~1)\")\n",
    "\n",
    "# VizualizÄƒm efectul standardizÄƒrii\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ãnainte de standardizare\n",
    "X_train.iloc[:, 0].hist(bins=30, ax=axes[0], color='#3498db', alpha=0.7)\n",
    "axes[0].set_title(f'ÃNAINTE: {X_train.columns[0]}', fontweight='bold')\n",
    "axes[0].set_xlabel('Valoare originalÄƒ')\n",
    "axes[0].set_ylabel('FrecvenÈ›Äƒ')\n",
    "\n",
    "# DupÄƒ standardizare\n",
    "X_train_scaled.iloc[:, 0].hist(bins=30, ax=axes[1], color='#2ecc71', alpha=0.7)\n",
    "axes[1].set_title(f'DUPÄ‚: {X_train.columns[0]} (Z-score)', fontweight='bold')\n",
    "axes[1].set_xlabel('Z-score')\n",
    "axes[1].set_ylabel('FrecvenÈ›Äƒ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smote"
   },
   "source": [
    "### âš–ï¸ Tratarea Imbalance-ului cu SMOTE (OpÈ›ional)\n",
    "\n",
    "**SMOTE** (Synthetic Minority Over-sampling Technique):\n",
    "- CreeazÄƒ exemple sintetice pentru clasa minoritarÄƒ (default)\n",
    "- FuncÈ›ioneazÄƒ prin interpolarea Ã®ntre vecini apropiaÈ›i\n",
    "\n",
    "**âš ï¸ NotÄƒ:** SMOTE se aplicÄƒ DOAR pe train set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apply_smote"
   },
   "outputs": [],
   "source": [
    "# AplicÄƒm SMOTE pentru a balansa clasele\n",
    "print(\"ğŸ”„ AplicÄƒm SMOTE pentru balansarea claselor...\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nğŸ“Š DistribuÈ›ia ÃNAINTE de SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nğŸ“Š DistribuÈ›ia DUPÄ‚ SMOTE:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "\n",
    "# Vizualizare comparaÈ›ie\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "pd.Series(y_train).value_counts().plot.bar(\n",
    "    ax=axes[0], color=['#2ecc71', '#e74c3c'], alpha=0.7\n",
    ")\n",
    "axes[0].set_title('ÃNAINTE de SMOTE', fontweight='bold')\n",
    "axes[0].set_xlabel('ClasÄƒ')\n",
    "axes[0].set_ylabel('NumÄƒr de exemple')\n",
    "\n",
    "pd.Series(y_train_balanced).value_counts().plot.bar(\n",
    "    ax=axes[1], color=['#2ecc71', '#e74c3c'], alpha=0.7\n",
    ")\n",
    "axes[1].set_title('DUPÄ‚ SMOTE', fontweight='bold')\n",
    "axes[1].set_xlabel('ClasÄƒ')\n",
    "axes[1].set_ylabel('NumÄƒr de exemple')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… SMOTE aplicat cu succes!\")\n",
    "print(\"\\nğŸ’¡ Vom antrena modele pe AMBELE seturi (original È™i SMOTE) pentru comparaÈ›ie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "models"
   },
   "source": [
    "## 5ï¸âƒ£ Construirea Modelelor de Machine Learning\n",
    "\n",
    "### ğŸ“š Modelele Pe Care Le Vom Antrena:\n",
    "\n",
    "1. **Logistic Regression**\n",
    "   - Model liniar pentru clasificare binarÄƒ\n",
    "   - ReturneazÄƒ probabilitÄƒÈ›i Ã®ntre 0 È™i 1\n",
    "   - Avantaje: Rapid, interpretabil (coeficienÈ›i explicabili)\n",
    "\n",
    "2. **Linear Discriminant Analysis (LDA)**\n",
    "   - GÄƒseÈ™te combinaÈ›ia liniarÄƒ de features care separÄƒ cel mai bine clasele\n",
    "   - Presupune distribuÈ›ie normalÄƒ a datelor\n",
    "   - Avantaje: Reducere de dimensionalitate built-in\n",
    "\n",
    "3. **K-Nearest Neighbors (KNN)**\n",
    "   - ClasificÄƒ bazat pe \"vecinii\" apropiaÈ›i Ã®n spaÈ›iul features\n",
    "   - FoloseÈ™te distanÈ›a EuclideanÄƒ (de aceea standardizarea e crucialÄƒ!)\n",
    "   - Avantaje: Non-parametric, flexibil\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "logistic"
   },
   "source": [
    "### 1ï¸âƒ£ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_logistic"
   },
   "outputs": [],
   "source": [
    "print(\"ğŸ”µ AntrenÄƒm Logistic Regression...\\n\")\n",
    "\n",
    "# Model 1: Pe date originale (imbalanced)\n",
    "log_reg_original = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'  # Ajustare automatÄƒ pentru imbalance\n",
    ")\n",
    "log_reg_original.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model 2: Pe date SMOTE (balanced)\n",
    "log_reg_smote = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "log_reg_smote.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# PredicÈ›ii\n",
    "y_pred_log_orig = log_reg_original.predict(X_test_scaled)\n",
    "y_pred_log_smote = log_reg_smote.predict(X_test_scaled)\n",
    "\n",
    "# ProbabilitÄƒÈ›i (pentru AUC)\n",
    "y_proba_log_orig = log_reg_original.predict_proba(X_test_scaled)[:, 1]\n",
    "y_proba_log_smote = log_reg_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"âœ… Logistic Regression antrenat!\")\n",
    "\n",
    "# VizualizÄƒm coeficienÈ›ii (interpretabilitate)\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coeficient': log_reg_original.coef_[0]\n",
    "}).sort_values('Coeficient', key=abs, ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in coefficients['Coeficient']]\n",
    "plt.barh(coefficients['Feature'], coefficients['Coeficient'], color=colors)\n",
    "plt.xlabel('Coeficient (Impact asupra probabilitÄƒÈ›ii de default)', fontsize=11)\n",
    "plt.title('Top 15 Features - Logistic Regression (Interpretabilitate)', fontweight='bold', fontsize=13)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Interpretarea CoeficienÈ›ilor:\\n\")\n",
    "print(\"ğŸ’¡ CoeficienÈ›i POZITIVI (ğŸŸ¢) = Cresc probabilitatea de DEFAULT\")\n",
    "print(\"   Exemplu: DacÄƒ 'duration_months' are coeficient pozitiv, Ã®nseamnÄƒ cÄƒ\")\n",
    "print(\"   Ã®mprumuturi pe termen mai lung â†’ risc mai mare de default\\n\")\n",
    "\n",
    "print(\"ğŸ’¡ CoeficienÈ›i NEGATIVI (ğŸ”´) = Scad probabilitatea de DEFAULT\")\n",
    "print(\"   Exemplu: DacÄƒ 'checking_status' are coeficient negativ, Ã®nseamnÄƒ cÄƒ\")\n",
    "print(\"   conturi cu sold mai mare â†’ risc mai mic de default\\n\")\n",
    "\n",
    "print(\"ğŸ” Top 5 factori de risc (valoare absolutÄƒ):\")\n",
    "for idx, row in coefficients.head(5).iterrows():\n",
    "    direction = \"â¬†ï¸ CREÈ˜TE\" if row['Coeficient'] > 0 else \"â¬‡ï¸ SCADE\"\n",
    "    print(f\"   {direction} riscul: {row['Feature']} (coef: {row['Coeficient']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lda"
   },
   "source": [
    "### 2ï¸âƒ£ Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_lda"
   },
   "outputs": [],
   "source": [
    "print(\"ğŸ”µ AntrenÄƒm Linear Discriminant Analysis (LDA)...\\n\")\n",
    "\n",
    "# Model 1: Pe date originale\n",
    "lda_original = LinearDiscriminantAnalysis()\n",
    "lda_original.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model 2: Pe date SMOTE\n",
    "lda_smote = LinearDiscriminantAnalysis()\n",
    "lda_smote.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# PredicÈ›ii\n",
    "y_pred_lda_orig = lda_original.predict(X_test_scaled)\n",
    "y_pred_lda_smote = lda_smote.predict(X_test_scaled)\n",
    "\n",
    "# ProbabilitÄƒÈ›i\n",
    "y_proba_lda_orig = lda_original.predict_proba(X_test_scaled)[:, 1]\n",
    "y_proba_lda_smote = lda_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"âœ… LDA antrenat!\")\n",
    "print(f\"\\nğŸ“Š Explained variance ratio: {lda_original.explained_variance_ratio_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knn"
   },
   "source": [
    "### 3ï¸âƒ£ K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Cum funcÈ›ioneazÄƒ KNN:**\n",
    "1. Pentru o nouÄƒ observaÈ›ie, gÄƒseÈ™te cele mai apropiate K puncte din train set\n",
    "2. ClasificÄƒ bazat pe \"votul\" majoritar al vecinilor\n",
    "3. DistanÈ›a este EuclideanÄƒ: `d = âˆšÎ£(xâ‚ - xâ‚‚)Â²`\n",
    "\n",
    "**âš ï¸ De aceea standardizarea este CRUCIALÄ‚ pentru KNN!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "find_best_k"
   },
   "outputs": [],
   "source": [
    "# GÄƒsim cel mai bun K folosind cross-validation\n",
    "print(\"ğŸ” CÄƒutÄƒm cel mai bun K pentru KNN...\\n\")\n",
    "\n",
    "k_range = range(1, 31, 2)  # TestÄƒm K-uri impare de la 1 la 29\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    scores = cross_val_score(\n",
    "        knn, X_train_scaled, y_train, \n",
    "        cv=5, \n",
    "        scoring='roc_auc'  # Folosim AUC pentru evaluare\n",
    "    )\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# GÄƒsim best K\n",
    "best_k = k_range[np.argmax(cv_scores)]\n",
    "print(f\"âœ… Cel mai bun K: {best_k} (AUC = {max(cv_scores):.4f})\")\n",
    "\n",
    "# Vizualizare\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, cv_scores, marker='o', linestyle='-', color='#3498db', linewidth=2)\n",
    "plt.xlabel('NumÄƒrul de Vecini (K)', fontsize=12)\n",
    "plt.ylabel('Cross-Validation AUC Score', fontsize=12)\n",
    "plt.title('GÄƒsirea Optimalului K pentru KNN', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=best_k, color='#e74c3c', linestyle='--', label=f'Best K = {best_k}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_knn"
   },
   "outputs": [],
   "source": [
    "print(f\"\\nğŸ”µ AntrenÄƒm KNN cu K={best_k}...\\n\")\n",
    "\n",
    "# Model 1: Pe date originale\n",
    "knn_original = KNeighborsClassifier(\n",
    "    n_neighbors=best_k,\n",
    "    metric='euclidean',\n",
    "    weights='distance'  # Vecini mai apropiaÈ›i au greutate mai mare\n",
    ")\n",
    "knn_original.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model 2: Pe date SMOTE\n",
    "knn_smote = KNeighborsClassifier(\n",
    "    n_neighbors=best_k,\n",
    "    metric='euclidean',\n",
    "    weights='distance'\n",
    ")\n",
    "knn_smote.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# PredicÈ›ii\n",
    "y_pred_knn_orig = knn_original.predict(X_test_scaled)\n",
    "y_pred_knn_smote = knn_smote.predict(X_test_scaled)\n",
    "\n",
    "# ProbabilitÄƒÈ›i\n",
    "y_proba_knn_orig = knn_original.predict_proba(X_test_scaled)[:, 1]\n",
    "y_proba_knn_smote = knn_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"âœ… KNN antrenat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6ï¸âƒ£ Evaluarea Modelelor\n",
    "\n",
    "### ğŸ“Š Metrici pentru Date Imbalanced\n",
    "\n",
    "**De ce acurateÈ›ea NU este suficientÄƒ?**\n",
    "\n",
    "Exemplu: DacÄƒ 70% din clienÈ›i NU sunt Ã®n default:\n",
    "- Un model \"prost\" care prezice ÃNTOTDEAUNA \"no default\" â†’ 70% acurateÈ›e!\n",
    "- Dar rateazÄƒ TOATE cazurile de default (cele pe care vrem sÄƒ le prindem!)\n",
    "\n",
    "**Metrici importante:**\n",
    "\n",
    "1. **Confusion Matrix:**\n",
    "   ```\n",
    "                  Predicted\n",
    "                0         1\n",
    "   Actual  0    TN        FP\n",
    "           1    FN        TP\n",
    "   ```\n",
    "\n",
    "2. **Precision:** TP / (TP + FP) - Din ce am prezis \"default\", cÃ¢te sunt cu adevÄƒrat?\n",
    "\n",
    "3. **Recall (Sensitivity):** TP / (TP + FN) - Din toÈ›i care sunt \"default\", pe cÃ¢È›i i-am prins?\n",
    "\n",
    "4. **F1-Score:** Media armonicÄƒ dintre Precision È™i Recall\n",
    "\n",
    "5. **AUC (Area Under ROC Curve):** MÄƒsoarÄƒ capacitatea de separare a claselor (0.5 = random, 1.0 = perfect)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_function"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    \"\"\"\n",
    "    FuncÈ›ie completÄƒ de evaluare pentru date imbalanced\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š EVALUARE MODEL: {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Metrici de bazÄƒ\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    print(\"ğŸ“ˆ METRICI:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f} â­\")\n",
    "    print(f\"   Recall:    {recall:.4f} â­\")\n",
    "    print(f\"   F1-Score:  {f1:.4f}\")\n",
    "    print(f\"   AUC:       {auc:.4f} â­â­â­\\n\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Confusion Matrix heatmap\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues', \n",
    "        xticklabels=['No Default', 'Default'],\n",
    "        yticklabels=['No Default', 'Default'],\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f'Confusion Matrix - {model_name}', fontweight='bold')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    axes[1].plot(fpr, tpr, color='#e74c3c', lw=2, label=f'AUC = {auc:.4f}')\n",
    "    axes[1].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate (Recall)')\n",
    "    axes[1].set_title(f'ROC Curve - {model_name}', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report detaliat\n",
    "    print(\"\\nğŸ“‹ CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=['No Default', 'Default']\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval_all"
   },
   "source": [
    "### ğŸ“Š Evaluarea Tuturor Modelelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_logistic"
   },
   "outputs": [],
   "source": [
    "# EvaluÄƒm Logistic Regression\n",
    "results_log_orig = evaluate_model(\n",
    "    y_test, y_pred_log_orig, y_proba_log_orig,\n",
    "    \"Logistic Regression (Original Data)\"\n",
    ")\n",
    "\n",
    "results_log_smote = evaluate_model(\n",
    "    y_test, y_pred_log_smote, y_proba_log_smote,\n",
    "    \"Logistic Regression (SMOTE)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_lda"
   },
   "outputs": [],
   "source": [
    "# EvaluÄƒm LDA\n",
    "results_lda_orig = evaluate_model(\n",
    "    y_test, y_pred_lda_orig, y_proba_lda_orig,\n",
    "    \"Linear Discriminant Analysis (Original Data)\"\n",
    ")\n",
    "\n",
    "results_lda_smote = evaluate_model(\n",
    "    y_test, y_pred_lda_smote, y_proba_lda_smote,\n",
    "    \"Linear Discriminant Analysis (SMOTE)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_knn"
   },
   "outputs": [],
   "source": [
    "# EvaluÄƒm KNN\n",
    "results_knn_orig = evaluate_model(\n",
    "    y_test, y_pred_knn_orig, y_proba_knn_orig,\n",
    "    f\"K-Nearest Neighbors (K={best_k}, Original Data)\"\n",
    ")\n",
    "\n",
    "results_knn_smote = evaluate_model(\n",
    "    y_test, y_pred_knn_smote, y_proba_knn_smote,\n",
    "    f\"K-Nearest Neighbors (K={best_k}, SMOTE)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## 7ï¸âƒ£ Compararea Modelelor\n",
    "\n",
    "### ğŸ† Care model este cel mai bun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_models"
   },
   "outputs": [],
   "source": [
    "# CreÄƒm DataFrame cu rezultatele\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Reg (Original)',\n",
    "        'Logistic Reg (SMOTE)',\n",
    "        'LDA (Original)',\n",
    "        'LDA (SMOTE)',\n",
    "        'KNN (Original)',\n",
    "        'KNN (SMOTE)'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        results_log_orig['accuracy'],\n",
    "        results_log_smote['accuracy'],\n",
    "        results_lda_orig['accuracy'],\n",
    "        results_lda_smote['accuracy'],\n",
    "        results_knn_orig['accuracy'],\n",
    "        results_knn_smote['accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        results_log_orig['precision'],\n",
    "        results_log_smote['precision'],\n",
    "        results_lda_orig['precision'],\n",
    "        results_lda_smote['precision'],\n",
    "        results_knn_orig['precision'],\n",
    "        results_knn_smote['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        results_log_orig['recall'],\n",
    "        results_log_smote['recall'],\n",
    "        results_lda_orig['recall'],\n",
    "        results_lda_smote['recall'],\n",
    "        results_knn_orig['recall'],\n",
    "        results_knn_smote['recall']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        results_log_orig['f1'],\n",
    "        results_log_smote['f1'],\n",
    "        results_lda_orig['f1'],\n",
    "        results_lda_smote['f1'],\n",
    "        results_knn_orig['f1'],\n",
    "        results_knn_smote['f1']\n",
    "    ],\n",
    "    'AUC': [\n",
    "        results_log_orig['auc'],\n",
    "        results_log_smote['auc'],\n",
    "        results_lda_orig['auc'],\n",
    "        results_lda_smote['auc'],\n",
    "        results_knn_orig['auc'],\n",
    "        results_knn_smote['auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† COMPARAÈšIA FINALÄ‚ A MODELELOR\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "display(comparison_df.style.highlight_max(axis=0, subset=['Precision', 'Recall', 'F1-Score', 'AUC'], color='lightgreen'))\n",
    "\n",
    "# Vizualizare comparativÄƒ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    comparison_df.plot(\n",
    "        x='Model', y=metric, kind='bar', \n",
    "        ax=ax, color=colors, legend=False\n",
    "    )\n",
    "    ax.set_title(f'ComparaÈ›ie: {metric}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# IdentificÄƒm cel mai bun model\n",
    "best_model_idx = comparison_df['AUC'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_model_auc = comparison_df.loc[best_model_idx, 'AUC']\n",
    "\n",
    "print(f\"\\nğŸ† CEL MAI BUN MODEL (bazat pe AUC): {best_model_name}\")\n",
    "print(f\"   AUC Score: {best_model_auc:.4f}\")\n",
    "print(f\"   Precision: {comparison_df.loc[best_model_idx, 'Precision']:.4f}\")\n",
    "print(f\"   Recall: {comparison_df.loc[best_model_idx, 'Recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roc_comparison"
   },
   "source": [
    "### ğŸ“ˆ Compararea ROC Curves pentru Toate Modelele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_all_roc"
   },
   "outputs": [],
   "source": [
    "# Plot toate ROC curves pe acelaÈ™i grafic\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "models_data = [\n",
    "    (y_proba_log_orig, 'Logistic Reg (Original)', '#3498db'),\n",
    "    (y_proba_log_smote, 'Logistic Reg (SMOTE)', '#2ecc71'),\n",
    "    (y_proba_lda_orig, 'LDA (Original)', '#e74c3c'),\n",
    "    (y_proba_lda_smote, 'LDA (SMOTE)', '#f39c12'),\n",
    "    (y_proba_knn_orig, 'KNN (Original)', '#9b59b6'),\n",
    "    (y_proba_knn_smote, 'KNN (SMOTE)', '#1abc9c')\n",
    "]\n",
    "\n",
    "for y_proba, name, color in models_data:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})', \n",
    "             color=color, lw=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ComparaÈ›ia ROC Curves - Toate Modelele', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretare:\")\n",
    "print(\"   - Curba mai aproape de colÈ›ul stÃ¢nga-sus = model mai bun\")\n",
    "print(\"   - AUC = 0.5 â†’ model random (linie diagonalÄƒ)\")\n",
    "print(\"   - AUC = 1.0 â†’ model perfect\")\n",
    "print(\"   - AUC > 0.7 â†’ model considerat bun Ã®n practicÄƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "business_impact"
   },
   "source": [
    "## 8ï¸âƒ£ AplicaÈ›ii Practice È™i Impact Business\n",
    "\n",
    "### ğŸ’¼ Cum se foloseÈ™te acest model Ã®n realitate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "score_example"
   },
   "outputs": [],
   "source": [
    "# SimulÄƒm scoring pentru clienÈ›i noi\n",
    "print(\"ğŸ¯ SIMULARE: Scoring clienÈ›i noi\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LuÄƒm 5 exemple random din test set\n",
    "sample_indices = np.random.choice(X_test_scaled.index, 5, replace=False)\n",
    "sample_data = X_test_scaled.loc[sample_indices]\n",
    "sample_labels = y_test.loc[sample_indices]\n",
    "\n",
    "# Folosim cel mai bun model pentru predicÈ›ie\n",
    "# Presupunem cÄƒ Logistic Regression (Original) este cel mai bun\n",
    "best_model = log_reg_original\n",
    "\n",
    "for idx, (sample_idx, sample) in enumerate(sample_data.iterrows(), 1):\n",
    "    # PredicÈ›ie probabilitate\n",
    "    proba = best_model.predict_proba(sample.values.reshape(1, -1))[0, 1]\n",
    "    prediction = best_model.predict(sample.values.reshape(1, -1))[0]\n",
    "    actual = sample_labels.loc[sample_idx]\n",
    "    \n",
    "    print(f\"\\nğŸ‘¤ CLIENT #{idx}\")\n",
    "    print(f\"   Risc de Default: {proba*100:.2f}%\")\n",
    "    print(f\"   PredicÈ›ie: {'âŒ DEFAULT' if prediction == 1 else 'âœ… NO DEFAULT'}\")\n",
    "    print(f\"   Realitate: {'âŒ DEFAULT' if actual == 1 else 'âœ… NO DEFAULT'}\")\n",
    "    print(f\"   Status: {'âœ… CORECT' if prediction == actual else 'âŒ GREÈ˜IT'}\")\n",
    "    \n",
    "    # Decizie de business\n",
    "    if proba < 0.3:\n",
    "        risk_level = \"ğŸŸ¢ RISC SCÄ‚ZUT\"\n",
    "        action = \"Aprobare automatÄƒ, ratÄƒ dobÃ¢ndÄƒ standard\"\n",
    "    elif proba < 0.6:\n",
    "        risk_level = \"ğŸŸ¡ RISC MEDIU\"\n",
    "        action = \"Revizie manualÄƒ, posibil ratÄƒ dobÃ¢ndÄƒ crescutÄƒ\"\n",
    "    else:\n",
    "        risk_level = \"ğŸ”´ RISC RIDICAT\"\n",
    "        action = \"Respingere sau condiÈ›ii stricte + garanÈ›ii\"\n",
    "    \n",
    "    print(f\"   {risk_level}\")\n",
    "    print(f\"   ğŸ“‹ AcÈ›iune recomandatÄƒ: {action}\")\n",
    "    print(\"   \" + \"-\"*55)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "threshold"
   },
   "source": [
    "### âš–ï¸ Ajustarea Threshold-ului de Decizie\n",
    "\n",
    "**Context Business:**\n",
    "- **Threshold Ã®nalt (ex: 0.7):** Mai puÈ›ine false positives â†’ Mai mulÈ›i clienÈ›i aprobaÈ›i, dar risc mai mare\n",
    "- **Threshold scÄƒzut (ex: 0.3):** Mai puÈ›ine false negatives â†’ Mai conservativ, dar pierdem clienÈ›i buni\n",
    "\n",
    "Alegerea threshold-ului depinde de **costurile de business**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "threshold_analysis"
   },
   "outputs": [],
   "source": [
    "# AnalizÄƒm diferite threshold-uri\n",
    "thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_proba_log_orig >= threshold).astype(int)\n",
    "    precisions.append(precision_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thresh, zero_division=0))\n",
    "\n",
    "# Vizualizare\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, precisions, 'o-', label='Precision', color='#3498db', linewidth=2)\n",
    "plt.plot(thresholds, recalls, 's-', label='Recall', color='#e74c3c', linewidth=2)\n",
    "plt.plot(thresholds, f1_scores, '^-', label='F1-Score', color='#2ecc71', linewidth=2)\n",
    "\n",
    "# GÄƒsim threshold optim (maxim F1)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "plt.axvline(x=optimal_threshold, color='black', linestyle='--', \n",
    "            label=f'Optimal Threshold = {optimal_threshold:.2f}')\n",
    "\n",
    "plt.xlabel('Decision Threshold', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Impactul Threshold-ului asupra Metricilor', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ Threshold Optim (maxim F1): {optimal_threshold:.2f}\")\n",
    "print(f\"   Precision la acest threshold: {precisions[optimal_idx]:.4f}\")\n",
    "print(f\"   Recall la acest threshold: {recalls[optimal_idx]:.4f}\")\n",
    "print(f\"   F1-Score la acest threshold: {f1_scores[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusions"
   },
   "source": [
    "## 9ï¸âƒ£ Concluzii È™i PaÈ™i UrmÄƒtori\n",
    "\n",
    "### âœ… Ce Am ÃnvÄƒÈ›at\n",
    "\n",
    "1. **Clasificare BinarÄƒ pentru Credit Scoring**\n",
    "   - Am construit modele care prezic probabilitatea de default\n",
    "   - Am folosit date reale din industrie (German Credit Data)\n",
    "\n",
    "2. **Tehnici de Machine Learning**\n",
    "   - **Logistic Regression:** Rapid È™i interpretabil\n",
    "   - **LDA:** Bun pentru separarea claselor cu distribuÈ›ii normale\n",
    "   - **KNN:** Flexibil, dar necesitÄƒ standardizare (z-scores)\n",
    "\n",
    "3. **Tratarea Date Imbalanced**\n",
    "   - SMOTE pentru balansarea claselor\n",
    "   - Metrici speciale: Precision, Recall, F1, AUC\n",
    "   - Confusion Matrix pentru Ã®nÈ›elegerea erorilor\n",
    "\n",
    "4. **ImportanÈ›a StandardizÄƒrii**\n",
    "   - Z-scores pentru KNN (distanÈ›Äƒ EuclideanÄƒ)\n",
    "   - Prevenirea dominaÈ›iei features cu scale mari\n",
    "\n",
    "### ğŸš€ PaÈ™i UrmÄƒtori (ÃmbunÄƒtÄƒÈ›iri Posibile)\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Crearea de noi features (ratios, interactions)\n",
    "   - SelecÈ›ia features-urilor cele mai importante\n",
    "\n",
    "2. **Modele Mai Avansate:**\n",
    "   - Random Forest\n",
    "   - Gradient Boosting (XGBoost, LightGBM)\n",
    "   - Neural Networks\n",
    "\n",
    "3. **Optimizare Hyperparameters:**\n",
    "   - Grid Search / Random Search\n",
    "   - Cross-validation mai robust (5-fold, 10-fold)\n",
    "\n",
    "4. **Cost-Sensitive Learning:**\n",
    "   - Definirea costurilor pentru FP vs FN\n",
    "   - Optimizarea pentru profit maxim, nu doar acurateÈ›e\n",
    "\n",
    "5. **Model Deployment:**\n",
    "   - Salvarea modelului (pickle, joblib)\n",
    "   - API pentru scoring Ã®n timp real\n",
    "   - Monitoring performanÈ›a Ã®n producÈ›ie\n",
    "\n",
    "### ğŸ“š Resurse Suplimentare\n",
    "\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/)\n",
    "- [Imbalanced-learn](https://imbalanced-learn.org/)\n",
    "- [Credit Scoring Best Practices](https://www.kaggle.com/learn/credit-risk-model-stability)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Mesaj Final\n",
    "\n",
    "Acest notebook demonstreazÄƒ un **workflow complet** pentru un proiect real de Data Science:\n",
    "1. âœ… Date reale (UCI Repository)\n",
    "2. âœ… EDA comprehensiv\n",
    "3. âœ… Preprocessing corect (stratification, standardization)\n",
    "4. âœ… Multiple modele (Logistic, LDA, KNN)\n",
    "5. âœ… Evaluare riguroasÄƒ pentru date imbalanced\n",
    "6. âœ… Interpretare business\n",
    "\n",
    "**Acesta este un proiect portfolio-ready pentru poziÈ›ii entry-level Ã®n Data Science!** ğŸ¯\n",
    "\n",
    "---\n",
    "\n",
    "*Creat pentru Ã®nvÄƒÈ›are È™i practicÄƒ Ã®n Machine Learning aplicat Ã®n domeniul financiar.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model"
   },
   "source": [
    "## ğŸ”Ÿ Bonus: Salvarea È™i ÃncÄƒrcarea Modelului"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_load"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# SalvÄƒm cel mai bun model\n",
    "model_filename = 'credit_risk_model.pkl'\n",
    "scaler_filename = 'credit_risk_scaler.pkl'\n",
    "\n",
    "joblib.dump(log_reg_original, model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "print(f\"âœ… Model salvat: {model_filename}\")\n",
    "print(f\"âœ… Scaler salvat: {scaler_filename}\")\n",
    "\n",
    "# Pentru a Ã®ncÄƒrca modelul ulterior:\n",
    "# loaded_model = joblib.load(model_filename)\n",
    "# loaded_scaler = joblib.load(scaler_filename)\n",
    "\n",
    "print(\"\\nğŸ’¾ Pentru deploy, ai nevoie de:\")\n",
    "print(\"   1. FiÈ™ierul cu modelul (.pkl)\")\n",
    "print(\"   2. FiÈ™ierul cu scaler-ul (.pkl)\")\n",
    "print(\"   3. Lista de features Ã®n ordinea corectÄƒ\")\n",
    "print(\"   4. Pipeline de preprocessing (encoding)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
